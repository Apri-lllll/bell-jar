---
title: "AI and Innovation"
date: "2025-01-26"
description: "Recycling my crap and giving it a home in this dumpster fire where it belongs."
tags: [research, essays, surveys]
---

> Last semester I poured many days and (mostly) nights into a section of a sprawling AI4Science perspective paper we were writing. But they ended up scrapping everything and writing a whole new piece of incoherent crap. Well my sweet crap needs a home.

Scientific innovation is a complex process encompassing diverse tasks, among which the practical implementation of many stages builds upon data and computation. Improving automation in these complete workflows presents unique barriers and opportunities compared to single scientific tasks. In this section, we emphasize how the role of AI within scientific hypothesis generation and validation can accelerate the advancement from simple discoveries to fundamental breakthroughs.
Through discussing various use cases, we aim to demonstrate how AI, drawing from established scientific practices, not only advances accepted scientific conventions but also pioneers entirely novel possibilities.

# The Progression of Scientific Discoveries

Scientific discovery is not simply a collection of disjointed findings, but rather a progressive process where surface-level observations are connected and generalized to induce more significant insights. We map out the above process into four representative hierarchies of scientific progress. First, isolated observations of natural phenomena spark research interest and guide further experimentation. Second, pattern recognition based on sufficient data begins to unify the observations and inspire the exploration of deeper mechanisms. Third, researchers collect and integrate heterogeneous evidence to formulate a theoretical framework underlying the observed patterns. Finally, these generalizable theories may give rise to cross-disciplinary applications and consequential changes in scientific paradigms.

This process has been consistently demonstrated in the development of many foundational theories that have had a lasting impact on modern scientific research. One example is the discovery of the central dogma, which outlines the common flow of genetic information between DNA, RNA, and proteins. This process of discovery started with initial observations of genetic inheritance trends. Most prominently, Morgan's fruit fly experiments uncovered hereditable traits and identified specific patterns that linked these traits to genes on chromosomes. Further experiments, represented by Avery's establishment of DNA as the fundamental genetic material and Chargaff's findings on DNA base composition, more explicitly investigated the mechanisms of genetic information flow. Meselson and Stahl, through tagging DNA with heavy atoms and observing resulting DNA patterns over generations, further elucidated DNA replication mechanisms. A host of such empirical observations across the stages of DNA replication, DNA transcription, and RNA translation led to the eventual theoretical unification of this process, first proposed by Francis Crick. Since its discovery, the central dogma has had momentous implications in science and engineering, giving rise to fields such as genetic engineering and synthetic biology. On the flipside, modern technologies and cross-disciplinary methods have also continually refined our understanding of the central dogma, uncovering novel mechanisms such as epigenetic regulation that were initially deemed impossible.

Although specific methodologies vary among scientific disciplines, two essential stages always form the process of ascending this scientific discovery hierarchy: a) proposing novel hypotheses from limited data and knowledge, and b) obtaining and analyzing further evidence to validate or correct the hypotheses. These steps form an iterative feedback loop that gradually transforms speculative hypotheses into established knowledge. This expanded knowledge base in turn facilitates further discoveries, gradually extending the boundaries of our scientific understanding.

# Hypothesis Generation: Balancing Concreteness and Creativity

AI's distinct advantages in extracting knowledge from large amounts of data are reshaping scientific hypothesis generation, which once relied heavily on human intuition and trial-and-error. AI models, capable of varying levels of autonomy, show promise across diverse scenarios.

## Constrained hypothesis generation

Constrained hypothesis generation involves using AI to model and solve well-defined scientific problems. With a specific end goal designated by human researchers, AI's advantages in data and computation greatly catalyze the problem-solving process, allowing models to efficiently navigate the vast space of possible candidate solutions. Domain-specific constraints and frameworks allow AI models to more effectively handle complex scientific problems, and human researchers to maintain high degrees of control over model behavior.

The above characteristics have ensured the success of AI-driven constrained hypothesis generation in data-abundant, computation-intensive scientific problems. In biology, computational protein design over both sequence and structure spaces are gaining traction. Recent protein design models can be conditioned over structural parameters, functional parameters, target structures, natural language descriptions, or combinations of multiple conditions, and therefore have a myriad of practical use cases. Additionally, AI methods ranging from variational inference to deep language models have enhanced gene regulatory network (GRN) inference. In chemistry, both function to material generation, exemplified by DeepMind's GNoME, and inverse function prediction through methods such as convolutional and graph neural networks, have been widely applied. This category of AI-assisted scientific innovation, though narrowly constrained, can yield high-throughput, high-quality hypotheses with direct practical value, often surpassing conventional methods in both efficiency and accuracy.

We also note the potential of these models to extrapolate from training data and reveal novel possibilities that in turn inspire human researchers. Continuing the discussion of AI models for protein design, an attractive direction is *de novo* protein generation, which bypasses the need for using natural protein structures as a starting point for design. Researchers have revealed that although protein models are mostly trained on natural proteins, they are able to generate novel protein structures outside natural distributions. Subsequent models tailored to *de novo* design have been extensively employed in scenarios such as designing diverse types of protein binders. A notable example is antibody design, where scientists are exploring structures entirely different from natural antibodies, including scFVs, nanobodies, and antibody mimetics, which can be designed to have wider ranges of functions and more desirable properties. AI-based protein design methods can potentially deliver higher reliability and throughput, accelerating scientific progress towards these next-generation antibodies. Such advancements demonstrate that AI models for constrained hypothesis generation do not merely parrot known trends, but can also contribute towards important novel discoveries.

## Open-ended hypothesis generation

Furthermore, scientific discovery demands not merely resolving predefined problems but also raising conducive questions and proposing valuable research directions, the latter arguably more challenging and pivotal. In response to the need for enhancing such open-ended hypothesis generation, AI-empowered tools, models, and systems are evolving from solving problems provided by humans to proposing and pursuing promising avenues of research fully automatically.

The advent of automated scientific research systems has been expedited by foundation models, which learn broadly applicable principles from vast training data and generalize across various downstream tasks. Most notably, large language models can form the central coordinator for many AI systems across diverse professional scenarios. LLMs are able to follow general instructions, employ domain knowledge in scientific settings, and flexibly utilize external tools. All of these features are crucial for LLMs to successfully manage components in an autonomous system to ensure that they work in concert towards the goal of scientific discovery. In addition, domain-specific foundation models have shown heightened capabilities in learning generalizable representations and facilitating downstream tasks. Examples are ubiquitous in realms such as omics, material science, Earth science, and physics. The generalization abilities of foundation models are irreplaceable in open-ended hypothesis generation, where comprehensive scientific understanding and versatile capabilities enable high-level innovations.

Shifting our focus from individual models to entire autonomous systems, current open-ended scientific systems commonly integrate general foundation models with specific professional knowledge and tools. Scientific literature and other knowledge bases can augment general language models to promote effective hypothesis proposal. The inclusion of highly-specified datasets, analytical methods, and professional tools further allow these systems to anchor novel hypotheses in empirical data analysis. CoScientist performs automatic chemistry research through integrating literature, web search, coding, and lab experiments. The Virtual Lab employed a team of AI agents to construct novel scientific pipelines consisting of multiple deep learning models, achieving automated nanobody design.

# Hypothesis Validation: The Synergy of AI and Conventional Methods

Compared with generation, hypothesis validation emphasizes drawing from different lines of evidence produced through complex analytical and experimental procedures. The progress of AI has accelerated and augmented existing practices, as well as introduced entirely novel approaches. AI's role in both theoretical predictions and experimental procedures has enabled multi-faceted hypothesis validation at high precision and speed.

## *In silico* validation

AI-driven computation, simulation, and prediction have increased the reliability of *in silico* validation, allowing it to better complement or even replace certain stages of traditional laboratory validation.

AI models can learn principles from real data and generalize them to novel hypothesis cases for *in silico* validation across multiple scales, modalities, and domains. Predictive AI models have been used to evaluate filter candidate protein designs based on objectives including protein fitness, thermostability, and binding affinity. Similarly, structure-to-function prediction in material sciences has provided valuable feedback signals for novel material engineering.

Aside from purely learning through data, integrating prior knowledge and constraints of the physical world contributes to the improved accuracy of simulating natural events. Coupling open-ended AI methods with structured prior knowledge, such as through knowledge graphs, allows models crucial access to established knowledge and evidence directly related to the hypothesis at hand. This class of structured constraints are also conducive to causal inference methods across applications such as modeling biological networks or chemical reaction pathways.

In addition, AI has shown promise in enhancing traditional simulation methods. Neural operators enable rapid, accurate solutions of complex differential equations that traditionally required intensive numerical methods. Molecular dynamics simulation is an integral method across many scientific directions, and AI's contribution to accelerating simulations without sacrificing accuracy will promote their productive use in the scientific discovery loop. For instance, AI$^2$BMD utilizes ML force fields to speed up biomolecular simulations by several orders of magnitude, with implications in protein conformation space exploration and protein property estimation.

Currently, *in silico* modeling is often still followed by experimentation in the physical world. However, preliminary results and proposals from AI simulations are valuable in precisely guiding further human efforts, directing resource-intensive procedures to the most promising candidates. Advances in such technologies can be game-changing for applications where experimental trials are especially time-consuming. For instance, in drug discovery, where typical clinical trials take multiple years to perform, improvements in AI-enhanced drug screening will significantly accelerate the path towards verified drugs.

## Experimental validation

In the realm of laboratory experiments, AI has also proved invaluable for streamlining and automating experimental procedures, improving standardization and efficiency. AI language or robotic models operate suitably designed laboratory equipment and platforms to achieve unprecedented levels of automation in laboratories. Components of these systems need to understand the high-level scientific task and objectives, plan detailed experimental procedures, meticulously execute the steps in the physical world, and navigate possible errors and interactions with human researchers.

Self-driving laboratories (SDLs) are notable products of the above technological developments. They are able to operate automatically over long periods of time, leveraging diverse tools and knowledge, and in this process efficiently validating data points across a vast hypothesis space. SDLs in synthetic chemistry are the most representative, where AI-enabled autonomous models conduct multi-step decision making and laboratory operations to synthesize target materials. A-lab employs active learning and joins literature with computation to synthesize inorganic powders. More recently,  integrate a synthesis platform into a broader laboratory environment through mobile robots that mimick human researchers. Similar methods have also made strides towards automating protein engineering. SAMPLE uses machine learning to navigate the protein fitness landscape and propose candidate designs, and employs self-driving robotic experimentation to test the candidates \textit{in vitro}, forming a full discovery loop of novel, functional protein structures. Besides increasing automation, AI has given rise to new methods of tackling challenges in conventional experimentation, expanding the boundaries of what experiments we deem feasible, as exemplified in its role in SDLs for space biology.

# Towards Autonomous Scientific Discoveries and Advancement

Ultimately, achieving highly automated scientific discovery hinges on integrating an array of technologies and ideas described in previous sections. Here we discuss this full life cycle of discovery holistically, highlighting noteworthy challenges and developments.

## From tools and tasks to systems and scientific discoveries

From the perspective of AI-driven components of scientific discovery systems, multiple AI models with different specialized roles are often necessary for handling the full scientific process. These separate components can either be joined in predetermined workflows or brought together flexibly through handing over to AI the task of orchestrating the whole system. In the latter case, the module coordinating the system is often an LLM, though the prospect of alternative solutions is compelling.

From the perspective of individual tasks that constitute the full scientific process, hypothesis generation and validation need to be strung together into rapid, efficient feedback loops, where new hypotheses lead to tailored experimental designs and evidence retrieval, and newly acquired evidence informs hypothesis refinement and advancement. State-of-the-art AI systems have already progressed towards encapsulating this full feedback loop and yielded valid results. In SAMPLE's SDL, an intelligent agent designs novel proteins (hypothesis generation), and a laboratory environment tests their biochemical properties (hypothesis validation). In AI research, the AI Scientist develops an automatic peer review process to critique academic papers generated by the system, facilitating iterative refinement loops that improved result quality.

## Both specificity and versatility are necessary for effective autonomous discovery systems

Within the complicated systems and processes described above, we emphasize the importance of both specificity and versatility.

Effective scientific research entails rigor, and significant discrepancies in data and research paradigms across fields further necessitate specialized models with optimal performance on their intended tasks. This is reflected across all scientific disciplines in efforts to improve accuracy of simulations, requirements for statistical significance, reproducibility validations of experimental procedures, etc. The requirement for accuracy in science makes AI methods that far surpass human abilities and traditional methods especially noteworthy, since concrete improvements in the performance of any single step in the research process may have ripple effects across all subsequent stages. For instance, valid proteomics downstream analysis is reliant on accurate peptide sequencing methods that ensure the correct peptides and proteins are interpreted from the raw outputs of protein sequencing techniques.

At the same time, however, advancing discoveries from observations of patterns and phenomena to more fundamental insights often requires versatility in dealing with disparate lines of knowledge in order to propose and validate higher-level hypotheses. More specifically, the role of cross-disciplinary knowledge in science is evident. In the case of discovering the central dogma and genetic code, Chargaff's establishment of patterns in DNA nucleotides was a pivotal stage. After encountering Avery's work establishing DNA as genetic material from a biological standpoint, Chargaff, who's educational background was in molecular chemistry, sought to dissect the origins of DNA diversity on a molecular level, expressing that he was "deeply moved by the sudden appearance of a giant bridge between chemistry and genetics." Decades later, we are now familiar with the cross-disciplinary realms of molecular biology and molecular genetics. Approaches to modeling the synergy between scientific directions will be especially significant against the backdrop of burgeoning cross-disciplinary inspiration, allowing AI to evolve in tandem with both established and emerging hybrid domains. Throughout developments in AI, we also see how multimodal and multitask learning bridge diverse information for AI models , enabling them to both construct cross-domain connections and reversely enhance performance on individual tasks. Recently, Evo, using DNA as a fundamental "language" to connect DNA, RNA, and proteins, showed capabilities in understanding and generating multimodal biological complexes, some proven to be functional in the real world.

Bringing together these two intuitively conflicting desiderata is a continuing area of interest. It is possible to instill a general AI model with specific abilities tailored to their downstream applications, either through well-designed fine-tuning pipelines that consciously maintain the model's generalizability, or through keeping the model itself intact and instead augmenting it with external tools and knowledge. Another avenue is to extract the best of both worlds through self-organizing, cooperative systems of multiple models taking on different roles. In this case, both the performance of individual components, which serve as the building blocks of discovery, and the efficient coordination of their interactions, forming the scaffold of the process, are essential.

## Challenges in ascending the hierarchy of discoveries and the role of AI

In previous sections, we have repeatedly emphasized the core challenge of ascending the hierarchy of scientific discoveries outlined in Section \ref{3-1}. This process will likely continue to be jointly propelled by both systematic, data-grounded deductions and bold, intuitive innovations. Undeniably, immense progress in both the scale of accessible high-quality data and our ability to effectively model them are enabling the former to assume a more central role across all disciplines. This area is also where the promise of AI is currently most palpable. These transformational developments, instead of undermining the value of the latter category of "leap-of-faith" innovations, establish more abundant empirical grounds for their formulation and are conducive to their success. Ascending the ladder towards significant scientific discoveries still calls for such creative, unbounded "thinking", which supports scientists to fully exploit the value of large-scale data and thoroughly decode their rich implications.

AI's role in along this line has been primarily demonstrated through their ability to formulate innovations through open-ended generation. Si et al. show that LLMs are capable of generating more novel ideas compared with human experts. We can see how exploratory innovations that cannot be achieved through straightforward, brute-force data computation can be elicited via the generalizability of AI, and certain superhuman results along this vein are reachable. A caveat is that these results are still confined to isolated ideas at their most primary stages, and often lack diversity. It remains to be seen whether developments in AI technologies can eventually culminate in more revolutionary breakthroughs.

With the advent of AI profoundly altering how humans conduct research and AI methods anchoring their indispensable roles in science, new paradigms of human-AI interaction are also emerging. Recent research is rapidly evolving AI from a tool entirely manipulated by human researchers into a research partner capable of independently planning research trajectories, or even to fully autonomous researchers or teams, though the latter possibilities are only budding. AI is not merely mimicking human research methods, but introducing new approaches that were previously infeasible and interpreting scientific information in manners exceeded full human comprehension. As a result, methods for interpreting and controlling these capable, autonomous, and innovative AI technologies are crucial.

> And finally, a conclusion written by a collaborator, just for the endless laughs:
> The fast development and employment of Artificial Intelligence in the Scientific Research process is shaping the way that researchers discovery and understand the world. While significant processes have been made across many disciplines, a formal definition and categorization of AI for Science are overlooked, which hinders the ideas and outcomes shared among relevant disciplines. In this paper, we discuss the intersections of AI and Science from a systemic methodological perspective. Considering the whole research cycle, we divide the AI for Science paradigm into three part, i.e., data frameworks, computation methodologies, and innovation paradigm. Based on this categorization, we then carefully and systematically summarize the challenges and representative works in each part. We hope this work can bridge the AI for Science researches scattered in different domains and disciplines and become an accelerator that speed up the foundation artificial intelligence researches that can benefit diverse scientific disciplines. 
